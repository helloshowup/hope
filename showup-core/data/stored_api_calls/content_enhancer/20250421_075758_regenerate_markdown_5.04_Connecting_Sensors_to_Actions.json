{
  "file": "5.04_Connecting_Sensors_to_Actions.md",
  "request_type": "regenerate_markdown",
  "markdown_text": "# 5.4\n# **Connecting Sensors to Actions**\n\n## Learning Objectives\n\nBy the end of this session, you'll be able to:\n- Explain how sensors help robots make choices\n- Use \"if-then\" rules to make a robot respond to its world\n- Test a robot program to make sure it works well\n## **Lesson Podcast Discussion: Enabling Robot Autonomy Through Sensors**\n\nThis podcast explores how sensor-based programming allows robots to make decisions independently, responding to their environment without human intervention.\n\n## **Sensor Inputs in Programming**\n\nIn traditional programming, we create sequences of instructions that execute exactly as written. However, robots that interact with their environment need to gather information about the world around them. This is where sensors become essential.\n\nSensors act as the \"eyes,\" \"ears,\" and \"sense of touch\" for robots. They convert physical phenomena like light, sound, or pressure into electrical signals that the robot's processor can understand. In programming terms, sensors provide the **inputs** that drive decision-making.\n\nThink about how you use your own senses. When you touch something hot, your brain quickly processes that information and tells your hand to pull away. Robots work in a similar way, but they need us to program these reactions.\n\nWithout sensors, a robot would be like a person trying to walk through a room with their eyes closed and ears plugged. It might follow instructions perfectly, but it couldn't adapt to anything unexpected in its path.\n\n### **The Input-Processing-Output Framework**\n\nEvery robotic system follows the **Input-Processing-Output (IPO)** framework:\n\n1. **Input**: Sensors collect data from the environment\n2. **Processing**: The robot's program interprets the data and makes decisions\n3. **Output**: Actuators (motors, lights, speakers) perform actions based on those decisions\n\nFor example, in a line-following robot:\n- **Input**: Light sensors detect the contrast between a black line and white background\n- **Processing**: The program determines if the robot is veering off the line\n- **Output**: Motors adjust speed to steer the robot back onto the line\n\nThis framework forms the foundation of all sensor-based programming.\n\nLet's look at another example: automatic doors at a grocery store. The door uses a motion sensor (input) to detect when someone approaches. The control system processes this information and decides the door should open (processing). Finally, the motors activate to slide the door open (output).\n\nA school security system works in a similar way. Motion sensors (input) detect movement in hallways after hours. The security system (processing) determines if this is unusual activity. Then, it might turn on lights or sound an alarm (output) to respond to the situation.\n\n## **Creating Sensor Response Programs**\n\nNow that we understand how sensors fit into the programming framework, let's examine how to write programs that respond to sensor inputs.\n\n### **Conditional Statements**\n\nThe most common way to handle sensor inputs is through **conditional statements**—typically \"if-then-else\" structures. These allow the robot to make decisions based on sensor readings.\n\nBasic structure:\n\n```\nif (sensor_value meets condition) {\n    do_something();\n} else {\n    do_something_else();\n}\n```\n\nFor example, a program for an obstacle-avoiding robot might look like:\n\n```\nif (distance_sensor < 10) {\n    turn_right();\n} else {\n    move_forward();\n}\n```\n\nThis simple program tells the robot: \"If there's an obstacle less than 10 centimeters away, turn right; otherwise, keep moving forward.\"\n\nConditional statements work like the decisions you make every day. If it's raining, you bring an umbrella. If it's not raining, you leave the umbrella at home. Robots make similar decisions, but they need us to write these rules in code.\n\nYou can also create more complex decisions by adding more conditions:\n\n```\nif (distance_sensor < 5) {\n    back_up();\n} else if (distance_sensor < 15) {\n    turn_right();\n} else {\n    move_forward();\n}\n```\n\nThis program gives the robot three possible actions depending on how close an obstacle is.\n\n### **Threshold Values**\n\nWhen working with sensors, we often need to determine appropriate **\"threshold values\"** that trigger different actions. These thresholds depend on:\n\n1. The specific sensor being used\n2. The environment the robot operates in\n3. The desired behavior of the robot\n\nFor instance, a light sensor might return values from 0 (complete darkness) to 1023 (bright light). You might set a threshold of 500, where values below indicate a dark line, and values above indicate a light background.\n\nFinding the right threshold often takes experimentation. If you set a light sensor threshold too high or too low, your line-following robot might not detect the line correctly. The perfect threshold depends on the lighting in the room and the contrast between the line and the background.\n\nThink of thresholds like the temperature setting on a thermostat. If you set it to 70°F, the heater turns on when the temperature drops below 70°F and turns off when it rises above 70°F. The threshold (70°F) determines when the action changes.\n\n## **Testing Sensor-Based Programs**\n\nCreating a sensor-based program is only the first step. Testing and refining these programs ensures reliable robot behavior.\n\n### **Systematic Testing Approaches**\n\nTo effectively test sensor-based programs:\n\n1. **Start with controlled inputs**: Begin by manually activating sensors to verify basic functionality\n2. **Test edge cases**: Check behavior at the boundaries of your threshold values\n3. **Create realistic test scenarios**: Test your robot in conditions similar to its intended environment\n4. **Incremental development**: Start with simple behaviors and build complexity gradually\n\nWhen testing a robot with a distance sensor, for example, you might first place an object exactly at your threshold distance (like 15cm) to see if the robot responds correctly. Then try moving the object slightly closer and slightly farther to test the boundaries of your program's decision-making.\n\nIt's also important to test in different lighting conditions if you're using light sensors, or on different surfaces if you're using touch sensors. The more thoroughly you test, the more reliable your robot will be when faced with real-world situations.\n\nA good testing plan might look like this:\n- Test each sensor individually before combining them\n- Test on different surfaces (carpet, tile, wood)\n- Test in different lighting conditions (bright, dim, natural light)\n- Test with different obstacles (soft objects, hard objects, different shapes)\n- Test with multiple obstacles at once\n\nRemember that testing isn't just about finding problems—it's about making your robot smarter and more reliable. Each test helps you refine your program and improve your robot's performance.\n\n---stopandreflect---\n**CHECKPOINT:** Consider how a robot that follows pre-programmed instructions differs from one that responds to sensor inputs. How does the addition of sensors transform what the robot can accomplish and how it interacts with the world around it?\n---stopandreflectEND---\n\n## **Common Sensor Programming Challenges**\n\nEven experienced roboticists face challenges when creating sensor-based programs. Understanding these common issues can help you avoid or resolve them.\n\n### **Sensor Reliability Issues**\n\nSensors don't always provide consistent readings. Factors that can affect sensor reliability include:\n\n1. **Environmental conditions**: Lighting, temperature, and humidity can affect sensor performance\n2. **Calibration drift**: Sensors may need regular recalibration to maintain accuracy\n3. **Power fluctuations**: Changes in battery voltage can affect sensor readings\n\nTo address these issues, robust programs often include:\n- Calibration routines that run when the robot starts\n- Averaging multiple readings to reduce noise\n- Built-in tolerance for minor variations in sensor values\n\nFor example, instead of reading a light sensor just once, your program might take five readings and average them together. This helps filter out random fluctuations that could cause your robot to make incorrect decisions.\n\nAnother common technique is to add a small \"buffer zone\" around your threshold values. Instead of triggering an action exactly at a threshold of 500, you might only change behavior if the value goes below 480 or above 520. This prevents the robot from rapidly switching between two behaviors when sensor values hover near the threshold.\n\n### **Sensor Calibration**\n\nSensors often need calibration to work correctly in different environments. Calibration is like teaching your robot what \"normal\" looks like so it can detect when something changes.\n\nFor a line-following robot, calibration might involve:\n1. Holding the robot over the white background and recording the light sensor value\n2. Holding the robot over the black line and recording the light sensor value\n3. Setting the threshold halfway between these two values\n\nMany robots perform a quick calibration routine when they first turn on. This helps them adjust to the specific lighting and conditions of their environment. Without calibration, a robot that worked perfectly in your classroom might fail completely in a different room with brighter or dimmer lighting.\n\n### **Debugging Sensor Programs**\n\nWhen your sensor-based program isn't working as expected, try these debugging approaches:\n\n1. **Isolate components**: Test sensors independently from the rest of the program\n2. **Print sensor values**: Output sensor readings to understand what the robot is \"seeing\"\n3. **Simplify the program**: Start with basic functionality before adding complexity\n4. **Check thresholds**: Ensure your threshold values are appropriate for your environment\n\nOne of the most useful debugging techniques is to display sensor values on a screen or through console output. This lets you see exactly what information your robot is receiving. For instance, if your light sensor is reading 300 when you expected 700, you might need to adjust your lighting or recalibrate the sensor.\n\nRemember that debugging is a normal part of programming. Even professional roboticists spend a lot of time testing and fixing their code. Each problem you solve helps you become a better programmer and roboticist.\n\n---stopandreflect---\n**CHECKPOINT:** Think about everyday devices that use sensors to trigger actions (automatic doors, smart thermostats, or motion-activated lights). How might the functionality of these devices be improved with more sophisticated sensor programming?\n---stopandreflectEND---\n\n---checkyourunderstanding---\nWhich programming approach would be best for a robot that needs to avoid obstacles?\n\nA. A fixed movement sequence programmed in advance\n\nB. A random movement generator\n\nC. A program that responds to touch or distance sensor inputs\n\nD. A program that only works when controlled by a human\n---answer---\nThe correct answer is C. A program that responds to touch or distance sensor inputs. For obstacle avoidance, the robot needs to sense obstacles (input) and change its movement (output) accordingly, which requires sensor-based programming. If you chose A, this approach wouldn't work because the robot couldn't adapt to unpredictable obstacles. If you chose B, random movements wouldn't efficiently avoid obstacles. If you chose D, the robot wouldn't be autonomous and would require constant human monitoring.\n---answerEND---\n---checkyourunderstandingEND---",
  "instructions": "Content Enhancement Prompt for Instructional Material\nYou are an expert instructional designer specializing in content enhancement. Your task is to expand the provided lesson draft by approximately 15% with meaningful additions that will increase learner engagement and retention.\nYour approach should include:\n\nAnalyze the target learner profile carefully before making any additions\nPreserve the original content structure and flow\nFocus on enhancing rather than merely extending content\n\nAdd only the following high-value elements:\n\nConcrete, relevant examples that illustrate abstract concepts\nBrief scenario-based applications that show practical relevance\nContextual connections to real-world applications\nMetaphors or analogies that clarify complex ideas\nSupporting evidence or data points that reinforce key concepts\n\nImportant guidelines:\n\nMaintain the original voice and tone\nEnsure all additions directly support the stated learning objectives\nDistribute enhancements evenly throughout the content\nAvoid redundancy, fluff, or unnecessarily complex language\nDo NOT add additional reflective questions or knowledge checks as these already exist\nDo NOT edit or modify any existing markers or structural elements such as ---checkyourunderstanding--- or similar formatting tags\nAvoid overused narrative devices or AI writing clichés (e.g., detective scenarios, journeys, characters named Alex/Sam)\nRemember that this content is meant to be read, not heard - optimize for readability and visual processing\nUse the target learner profile to guide the professional context of your examples\n\nThe final output should feel seamlessly integrated with the original content while providing additional value through strategically placed, contextually appropriate enhancements.",
  "context": "# Preparatory Context for Content Enhancement\n\n## Content Summary\nThe current content provides an overview of how sensors enable robots to make autonomous decisions by gathering environmental data (inputs), processing that information, and triggering appropriate actions (outputs). It covers the fundamental Input-Processing-Output framework, the use of conditional statements and threshold values to program sensor-based responses, and common challenges in sensor-based programming such as reliability issues and calibration. The content aims to equip learners with a conceptual understanding of sensor integration in robotics.\n\n## Enhancement Requirements\nThe enhancement prompt calls for expanding the existing lesson draft by approximately 15% through the addition of high-value elements that will increase learner engagement and retention. This includes:\n- Concrete, relevant examples that illustrate abstract concepts\n- Brief scenario-based applications that show practical relevance\n- Contextual connections to real-world applications\n- Metaphors or analogies that clarify complex ideas\n- Supporting evidence or data points that reinforce key concepts\n\nThe enhancements should maintain the original voice and tone, support the stated learning objectives, and be distributed evenly throughout the content.\n\n## Target Learner Considerations\nThe target audience for this content is 11-14 year old students (grades 6-8) with varying exposure to STEM concepts and potentially their first formal introduction to robotics. Key considerations for this learner profile include:\n\n- Language and Readability: Keep content at a 5th-6th grade reading level, using everyday words, active voice, and short sentences.\n- Structure and Organization: Use frequent headings, subheadings, bullet points, and visual breaks to create clear \"chunks\" of information.\n- Visual Elements: Include relevant images, diagrams, and labeled illustrations to support understanding.\n- Engagement Strategies: Connect to real-world examples familiar to middle schoolers, use a conversational tone, and incorporate \"Stop and Think\" moments.\n\n## Learning Medium Considerations\nThis content is intended for asynchronous online delivery as part of Excel Middle School's curriculum, with no peer interaction. As such, the enhancements should optimize for independent, self-paced learning on a digital platform. Formatting, media integration, and interactive elements should be considered to enhance the learner experience within the constraints of the online medium.\n\n## Key Considerations\n- Preserve the original content's core purpose, structure, and flow while strategically enhancing it.\n- Ensure all additions directly support the stated learning objectives and provide meaningful value to the learner.\n- Avoid redundancy, fluff, or unnecessarily complex language that could overwhelm the target audience.\n- Leverage the target learner profile to guide the context and relevance of examples, metaphors, and applications.\n- Optimize for the asynchronous online delivery medium, considering formatting, visual elements, and interactive features.\n\n## Suggested Approach\n1. Carefully review the current content to fully understand the key themes, concepts, and learning objectives.\n2. Analyze the enhancement prompt to identify the specific types of high-value elements to incorporate.\n3. Refer to the target learner profile to determine the appropriate language, complexity, and contextual references for the additions.\n4. Brainstorm relevant, engaging examples, scenarios, analogies, and data points that align with the learning objectives and target audience.\n5. Strategically integrate the enhancements throughout the content, ensuring they build upon and reinforce the original material.\n6. Format the additions to seamlessly blend with the existing content, considering visual design, structure, and interactive elements suitable for the online medium.\n7. Review the final enhanced content to ensure it maintains educational integrity, addresses the prompt requirements, and provides a cohesive, engaging learning experience for the target audience.",
  "temperature": 0.2608695652173913,
  "response": "# 5.4\n# **Connecting Sensors to Actions**\n\n## Learning Objectives\n\nBy the end of this session, you'll be able to:\n- Explain how sensors help robots make choices\n- Use \"if-then\" rules to make a robot respond to its world\n- Test a robot program to make sure it works well\n## **Lesson Podcast Discussion: Enabling Robot Autonomy Through Sensors**\n\nThis podcast explores how sensor-based programming allows robots to make decisions independently, responding to their environment without human intervention.\n\n## **Sensor Inputs in Programming**\n\nIn traditional programming, we create sequences of instructions that execute exactly as written. However, robots that interact with their environment need to gather information about the world around them. This is where sensors become essential.\n\nSensors act as the \"eyes,\" \"ears,\" and \"sense of touch\" for robots. They convert physical phenomena like light, sound, or pressure into electrical signals that the robot's processor can understand. In programming terms, sensors provide the **inputs** that drive decision-making.\n\nThink about how you use your own senses. When you touch something hot, your brain quickly processes that information and tells your hand to pull away. Robots work in a similar way, but they need us to program these reactions.\n\nWithout sensors, a robot would be like a person trying to walk through a room with their eyes closed and ears plugged. It might follow instructions perfectly, but it couldn't adapt to anything unexpected in its path.\n\n### **The Input-Processing-Output Framework**\n\nEvery robotic system follows the **Input-Processing-Output (IPO)** framework:\n\n1. **Input**: Sensors collect data from the environment\n2. **Processing**: The robot's program interprets the data and makes decisions\n3. **Output**: Actuators (motors, lights, speakers) perform actions based on those decisions\n\nFor example, in a line-following robot:\n- **Input**: Light sensors detect the contrast between a black line and white background\n- **Processing**: The program determines if the robot is veering off the line\n- **Output**: Motors adjust speed to steer the robot back onto the line\n\nThis framework forms the foundation of all sensor-based programming.\n\nLet's look at another example: automatic doors at a grocery store. The door uses a motion sensor (input) to detect when someone approaches. The control system processes this information and decides the door should open (processing). Finally, the motors activate to slide the door open (output).\n\nA school security system works in a similar way. Motion sensors (input) detect movement in hallways after hours. The security system (processing) determines if this is unusual activity. Then, it might turn on lights or sound an alarm (output) to respond to the situation.\n\nThink of the IPO framework like a digital version of your own senses and reactions. When you're riding a bike and see a pothole (input), your brain recognizes the danger (processing), and your hands turn the handlebars to avoid it (output). Robots follow this same pattern, just with electronic sensors instead of human senses.\n\n## **Creating Sensor Response Programs**\n\nNow that we understand how sensors fit into the programming framework, let's examine how to write programs that respond to sensor inputs.\n\n### **Conditional Statements**\n\nThe most common way to handle sensor inputs is through **conditional statements**—typically \"if-then-else\" structures. These allow the robot to make decisions based on sensor readings.\n\nBasic structure:\n\n```\nif (sensor_value meets condition) {\n    do_something();\n} else {\n    do_something_else();\n}\n```\n\nFor example, a program for an obstacle-avoiding robot might look like:\n\n```\nif (distance_sensor < 10) {\n    turn_right();\n} else {\n    move_forward();\n}\n```\n\nThis simple program tells the robot: \"If there's an obstacle less than 10 centimeters away, turn right; otherwise, keep moving forward.\"\n\nConditional statements work like the decisions you make every day. If it's raining, you bring an umbrella. If it's not raining, you leave the umbrella at home. Robots make similar decisions, but they need us to write these rules in code.\n\nYou can also create more complex decisions by adding more conditions:\n\n```\nif (distance_sensor < 5) {\n    back_up();\n} else if (distance_sensor < 15) {\n    turn_right();\n} else {\n    move_forward();\n}\n```\n\nThis program gives the robot three possible actions depending on how close an obstacle is.\n\nIn real-world applications, NASA's Mars rovers use conditional statements with their sensors to navigate the Martian terrain. When their cameras and distance sensors detect a large rock or steep slope, conditional statements in their programming tell them to find a safer path.\n\n### **Threshold Values**\n\nWhen working with sensors, we often need to determine appropriate **\"threshold values\"** that trigger different actions. These thresholds depend on:\n\n1. The specific sensor being used\n2. The environment the robot operates in\n3. The desired behavior of the robot\n\nFor instance, a light sensor might return values from 0 (complete darkness) to 1023 (bright light). You might set a threshold of 500, where values below indicate a dark line, and values above indicate a light background.\n\nFinding the right threshold often takes experimentation. If you set a light sensor threshold too high or too low, your line-following robot might not detect the line correctly. The perfect threshold depends on the lighting in the room and the contrast between the line and the background.\n\nThink of thresholds like the temperature setting on a thermostat. If you set it to 70°F, the heater turns on when the temperature drops below 70°F and turns off when it rises above 70°F. The threshold (70°F) determines when the action changes.\n\nThreshold values are crucial in many technologies you use daily. For example, your smartphone's auto-brightness feature uses a light sensor with thresholds to determine when to adjust screen brightness. When light levels fall below a certain threshold, the screen dims automatically to save battery and reduce eye strain.\n\n## **Testing Sensor-Based Programs**\n\nCreating a sensor-based program is only the first step. Testing and refining these programs ensures reliable robot behavior.\n\n### **Systematic Testing Approaches**\n\nTo effectively test sensor-based programs:\n\n1. **Start with controlled inputs**: Begin by manually activating sensors to verify basic functionality\n2. **Test edge cases**: Check behavior at the boundaries of your threshold values\n3. **Create realistic test scenarios**: Test your robot in conditions similar to its intended environment\n4. **Incremental development**: Start with simple behaviors and build complexity gradually\n\nWhen testing a robot with a distance sensor, for example, you might first place an object exactly at your threshold distance (like 15cm) to see if the robot responds correctly. Then try moving the object slightly closer and slightly farther to test the boundaries of your program's decision-making.\n\nIt's also important to test in different lighting conditions if you're using light sensors, or on different surfaces if you're using touch sensors. The more thoroughly you test, the more reliable your robot will be when faced with real-world situations.\n\nA good testing plan might look like this:\n- Test each sensor individually before combining them\n- Test on different surfaces (carpet, tile, wood)\n- Test in different lighting conditions (bright, dim, natural light)\n- Test with different obstacles (soft objects, hard objects, different shapes)\n- Test with multiple obstacles at once\n\nRemember that testing isn't just about finding problems—it's about making your robot smarter and more reliable. Each test helps you refine your program and improve your robot's performance.\n\nDid you know that before self-driving cars hit the road, companies like Tesla and Waymo test their sensor systems for millions of miles? They create detailed virtual simulations and controlled test tracks to ensure their vehicles can handle everything from rain and snow to unexpected pedestrians and road construction. This extensive testing is why these vehicles can navigate complex environments safely.\n\n---stopandreflect---\n**CHECKPOINT:** Consider how a robot that follows pre-programmed instructions differs from one that responds to sensor inputs. How does the addition of sensors transform what the robot can accomplish and how it interacts with the world around it?\n---stopandreflectEND---\n\n## **Common Sensor Programming Challenges**\n\nEven experienced roboticists face challenges when creating sensor-based programs. Understanding these common issues can help you avoid or resolve them.\n\n### **Sensor Reliability Issues**\n\nSensors don't always provide consistent readings. Factors that can affect sensor reliability include:\n\n1. **Environmental conditions**: Lighting, temperature, and humidity can affect sensor performance\n2. **Calibration drift**: Sensors may need regular recalibration to maintain accuracy\n3. **Power fluctuations**: Changes in battery voltage can affect sensor readings\n\nTo address these issues, robust programs often include:\n- Calibration routines that run when the robot starts\n- Averaging multiple readings to reduce noise\n- Built-in tolerance for minor variations in sensor values\n\nFor example, instead of reading a light sensor just once, your program might take five readings and average them together. This helps filter out random fluctuations that could cause your robot to make incorrect decisions.\n\nAnother common technique is to add a small \"buffer zone\" around your threshold values. Instead of triggering an action exactly at a threshold of 500, you might only change behavior if the value goes below 480 or above 520. This prevents the robot from rapidly switching between two behaviors when sensor values hover near the threshold.\n\nIn a school science lab, you might experience similar issues with electronic thermometers. If you've ever noticed a digital thermometer showing slightly different temperatures each time you use it, you're seeing sensor reliability issues in action. Scientists solve this by taking multiple measurements and finding the average, just like roboticists do with their sensors.\n\n### **Sensor Calibration**\n\nSensors often need calibration to work correctly in different environments. Calibration is like teaching your robot what \"normal\" looks like so it can detect when something changes.\n\nFor a line-following robot, calibration might involve:\n1. Holding the robot over the white background and recording the light sensor value\n2. Holding the robot over the black line and recording the light sensor value\n3. Setting the threshold halfway between these two values\n\nMany robots perform a quick calibration routine when they first turn on. This helps them adjust to the specific lighting and conditions of their environment. Without calibration, a robot that worked perfectly in your classroom might fail completely in a different room with brighter or dimmer lighting.\n\nCalibration is essential in many technologies. For instance, touchscreen tablets need to calibrate their touch sensors to respond accurately to your finger movements. When you set up a new tablet, it might ask you to tap specific points on the screen—this is the calibration process that ensures the device correctly interprets where you're touching.\n\n### **Debugging Sensor Programs**\n\nWhen your sensor-based program isn't working as expected, try these debugging approaches:\n\n1. **Isolate components**: Test sensors independently from the rest of the program\n2. **Print sensor values**: Output sensor readings to understand what the robot is \"seeing\"\n3. **Simplify the program**: Start with basic functionality before adding complexity\n4. **Check thresholds**: Ensure your threshold values are appropriate for your environment\n\nOne of the most useful debugging techniques is to display sensor values on a screen or through console output. This lets you see exactly what information your robot is receiving. For instance, if your light sensor is reading 300 when you expected 700, you might need to adjust your lighting or recalibrate the sensor.\n\nRemember that debugging is a normal part of programming. Even professional roboticists spend a lot of time testing and fixing their code. Each problem you solve helps you become a better programmer and roboticist.\n\nAccording to a survey of professional programmers, they typically spend about 50% of their time debugging code rather than writing new code. This shows that troubleshooting is a normal and important part of the development process, not just for beginners but for experts too!\n\n---stopandreflect---\n**CHECKPOINT:** Think about everyday devices that use sensors to trigger actions (automatic doors, smart thermostats, or motion-activated lights). How might the functionality of these devices be improved with more sophisticated sensor programming?\n---stopandreflectEND---\n\n---checkyourunderstanding---\nWhich programming approach would be best for a robot that needs to avoid obstacles?\n\nA. A fixed movement sequence programmed in advance\n\nB. A random movement generator\n\nC. A program that responds to touch or distance sensor inputs\n\nD. A program that only works when controlled by a human\n---answer---\nThe correct answer is C. A program that responds to touch or distance sensor inputs. For obstacle avoidance, the robot needs to sense obstacles (input) and change its movement (output) accordingly, which requires sensor-based programming. If you chose A, this approach wouldn't work because the robot couldn't adapt to unpredictable obstacles. If you chose B, random movements wouldn't efficiently avoid obstacles. If you chose D, the robot wouldn't be autonomous and would require constant human monitoring.\n---answerEND---\n---checkyourunderstandingEND---"
}